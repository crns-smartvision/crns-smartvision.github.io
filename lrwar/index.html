<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LRW-AR">
  <meta name="keywords" content="LipReading, LRW-AR, LRW, Arabic">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LRW-AR</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Samar Daou<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://abenhamadou.github.io">Achraf Ben-Hamadou</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rekikamed.github.io">Ahmed Rekik</a><sup>1,2</sup>,</span>
            <span class="author-block">
              Abdelaziz Kallel<sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Laboratory of Signals, systeMs, aRtificial Intelligence and neTworkS,</span>
            <span class="author-block"><sup>2</sup>Digital Research Centre of Sfax, Tunisia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- arxiv Link. -->
              <span class="link-block">
                <a href = "https://www.mdpi.com/2227-7080/13/1/26" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
              <!-- github Link. -->
              <span class="link-block">
                <a href="https://github.com/crns-smartvision/lrwar" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://osf.io/rz49x" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/lrwar.jpg"
           class="interpolation-image"
           alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
       A selection of speakers from the LRW-AR dataset showcasing diverse facial expressions and lip movements.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area. It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio speech recognition.
            Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours. However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector. To address this challenge, firstly, we propose a cross-attention fusion based approach for large lexicon Arabic vocabulary to predict spoken words in videos. Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region.
            Secondly, we introduce the first large-scale Arabic lipreading dataset (LRW-AR) containing 20,000 videos for 100-word classes, uttered by 36 speakers.
            The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words. Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



    <!-- Method overview. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method overview</h2>

<!--        &lt;!&ndash; Interpolating. &ndash;&gt;-->
<!--        <h3 class="title is-4">Input video</h3>-->
<!--        <div class="columns is-vcentered interpolation-panel">-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_start.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolate start reference image."/>-->
<!--            <p>Start Frame</p>-->
<!--          </div>-->
<!--          <div class="column interpolation-video-column">-->
<!--            <div id="interpolation-image-wrapper">-->
<!--              Loading...-->
<!--            </div>-->
<!--            <input class="slider is-fullwidth is-large is-info"-->
<!--                   id="interpolation-slider"-->
<!--                   step="1" min="0" max="10" value="0" type="range">-->
<!--          </div>-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_end.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolation end reference image."/>-->
<!--            <p class="is-bold">End Frame</p>-->
<!--          </div>-->
<!--        </div>-->
<!--        <br/>-->
<!--        &lt;!&ndash;/ Interpolating. &ndash;&gt;-->

        <div class="content has-text-justified">
          <p>
            Using a fusion method <span class="dnerf">FusionNet</span> with cross-attention networks, our approach enhances visual and geometric features integration for more accurate Arabic word prediction in lipreading. Additionally, the creation of LRW-AR, the first large-scale Arabic lipreading dataset, shows the effectiveness and feasibility of lipreading techniques in the Arabic language.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/architecture_web.jpg">
        </div>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">Video preprocessing:</span> crop the mouth region from the input video sequence and obtain the corresponding facial landmarks.
          </p>
          <p>
            <span class="dnerf">Visual-feature network:</span> extract relevant information from the preprocessed data.
          </p>
          <p>
            <span class="dnerf">Geometric-feature network:</span> encodes lip contour variation delivered by facial landmarks.
          </p>
          <p>
            <span class="dnerf">FusionNet network:</span> fuses the encoded features.
          </p>
          <p>
            <span class="dnerf">Sequence back-end network:</span> based on a multi-scale temporal convolutional network (MS-TCN) to encode temporal variation and classify the input video sequence.

          </p>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- LRW-AR dataset. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">LRW-AR Lipreading Dataset</h2>

        <div class="content has-text-justified">
          <p>
            We collect a naturally distributed large-scale lip reading dataset for Arabic language by scrapping videos from Youtube platform of talking people from News TV programs.
          </p>
          <p>
            Data processing was done via an automated pipeline :
          </p>

        </div>
        <div class="content has-text-centered">
          <img src="./static/images/data_collection.png">
        </div>
        <div class="content has-text-justified">
          <p>
            Using this pipeline we were able to obtain <strong> 100 classes </strong>, each corresponding to a different word.
          </p>
          <p>
            LRW-AR dataset involves a total of <strong> 36 speakers </strong>.
          </p>
          <p>
            Were each word was uttered <strong> 200 times </strong>, resulting in a total of <strong> 20,000 video samples </strong>.
          </p>

        </div>
        <!-- / LRW-AR dataset. -->

      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve">
              <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/111.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-chair-tp">
              <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/113.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-shiba">
              <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
                <source src="static/videos/112.mp4"
                        type="video/mp4">
              </video>

            </div>
            <div class="item item-blueshirt">
              <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/114.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-mask">
              <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/115.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-coffee">
              <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/116.mp4"
                        type="video/mp4">
              </video>
            </div>
            <div class="item item-toby">
              <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/117.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>

@Article{technologies13010026,
AUTHOR = {Daou, Samar and Ben-Hamadou, Achraf and Rekik, Ahmed and Kallel, Abdelaziz},
TITLE = {Cross-Attention Fusion of Visual and Geometric Features for Large-Vocabulary Arabic Lipreading},
JOURNAL = {Technologies},
VOLUME = {13},
YEAR = {2025},
NUMBER = {1},
ARTICLE-NUMBER = {26},
URL = {https://www.mdpi.com/2227-7080/13/1/26},
ISSN = {2227-7080},
DOI = {10.3390/technologies13010026}
}



    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         >
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link"  class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
